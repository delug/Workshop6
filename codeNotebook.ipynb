{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets training and testing data for MNIST dataset\n",
    "trainData = datasets.MNIST('data/', train = True, transform = transforms.ToTensor(), download=True)\n",
    "testData = datasets.MNIST('data/', train = False, transform = transforms.ToTensor(), download=True)\n",
    "\n",
    "#constructs loaders from datasets\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=100, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(testData, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convolutional Neural Network that transforms an 1x28x28 image to a 128x2x2 feature map\n",
    "'''\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        #(1,28,28) -> (16,24,24)\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        \n",
    "        #(16,12,12) -> (32,10,10)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        \n",
    "        #(32,5,5) -> (64,3,3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        \n",
    "        #(64,3,3) -> (64,3,3)\n",
    "        self.dropout = nn.Dropout2d(.2)\n",
    "        \n",
    "        #(64,3,3) -> (128,2,2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #(1,28,28) -> (16,24,24)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        \n",
    "        #(16,24,24) -> (16,12,12)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        \n",
    "        #(16,12,12) -> (32,10,10)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        \n",
    "        #(32,10,10) -> (32,5,5)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        \n",
    "        #(32,5,5) -> (64,3,3)\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        \n",
    "        #(64,3,3) -> (64,3,3)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #(64,3,3) -> (128,2,2)\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fully-Connected network that transforms 512 inputs to 10 softmaxed outputs\n",
    "'''\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        #512 -> 128\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        \n",
    "        #128 -> 10\n",
    "        self.linear2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        #512 -> 128\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        \n",
    "        #128 -> 10\n",
    "        x = F.softmax(self.linear2(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MNIST classifier composed of ConvNet and LinearNet\n",
    "'''\n",
    "class ClassifierNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifierNet, self).__init__()\n",
    "        \n",
    "        #(1,28,28) -> (128,2,2)\n",
    "        self.convNet = ConvNet()\n",
    "        \n",
    "        #512 -> 10\n",
    "        self.linearNet = LinearNet()\n",
    "    def forward(self, x):\n",
    "        x = self.convNet(x)\n",
    "        \n",
    "        x = x.view(-1, 512)\n",
    "        \n",
    "        x = self.linearNet(x)\n",
    "        \n",
    "        return x\n",
    "model = ClassifierNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimizer constructors\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "'''\n",
    "\n",
    "#Constructs a stochastic gradient descent optimizer with a learning rate of 1e-3 (0.001) on our model\n",
    "optimizer1 = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "#Constructs an Adam optimizer with a learning rate of 1e-3 on convolutional layers, 1e-2 on the linear layers of our model\n",
    "optimizer2 = optim.Adam([{'params': model.convNet.parameters()},\n",
    "                         {'params': model.linearNet.parameters(), 'lr': 1e-2}], lr = 1e-3)\n",
    "optimizer = optimizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Makes a training method, explaining the different components\n",
    "'''\n",
    "\n",
    "def train():\n",
    "    #initializes an accumulator to track the total loss over a training epoch\n",
    "    trainingLoss = 0\n",
    "    \n",
    "    #sets up a loop through the entire dataset\n",
    "    #index denotes the index of the batch being processed\n",
    "    #data is the batch of pictures to be processed through the network\n",
    "    #target is the corresponding batch of actual classification values we want to model\n",
    "    for index, (data, target) in enumerate(trainLoader):\n",
    "        #zeros out the gradient on the optimizer\n",
    "        #this prepares the optimizer to record weight updates for the new batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #runs the model on the data, storing the output\n",
    "        predictions = model(data)\n",
    "        \n",
    "        #calculates the loss/cost function for the batch, modeling the error on the current batch\n",
    "        batchLoss = F.cross_entropy(predictions, target)\n",
    "        \n",
    "        #backpropagates the loss through the network to determine how to update the weights of the network\n",
    "        batchLoss.backward()\n",
    "        \n",
    "        #adds the current batch's loss to the total loss for the training epoch\n",
    "        #.item() gets the actual numerical value for the batch's loss\n",
    "        trainingLoss += batchLoss.item()\n",
    "        \n",
    "        #updates the network's weights using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        #prints the average loss for a single image from the current batch every ten batches\n",
    "        if index % 10 == 0:\n",
    "            print(f\"Batch Loss: {batchLoss.item() / len(data)}\")\n",
    "            \n",
    "    #prints the total average loss for the training epoch\n",
    "    print(f\"Average Loss for Epoch: {trainingLoss / len(trainData)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Makes a testing method, explaining the different components\n",
    "'''\n",
    "\n",
    "def test():\n",
    "    #initializes an accumulator to track the total loss over a testing data\n",
    "    testingLoss = 0\n",
    "    \n",
    "    #initializes an accumulator to track how many images are processed\n",
    "    total = 0\n",
    "    \n",
    "    #initializes an accumulator to track how many images' values were correctly predicted\n",
    "    correct = 0\n",
    "    \n",
    "    #specifies that gradients should not be tracked through these observations\n",
    "    #this speeds up computation time for processes irrelevant to the training of the network\n",
    "    with torch.no_grad():\n",
    "        #sets up a loop through the entire dataset\n",
    "        #index denotes the index of the batch being processed\n",
    "        #data is the batch of pictures to be processed through the network\n",
    "        #target is the corresponding batch of actual classification values we want to model\n",
    "        for index, (data, target) in enumerate(testLoader):\n",
    "\n",
    "            #runs the model on the data, storing the output\n",
    "            predictions = model(data)\n",
    "\n",
    "            #calculates the loss/cost function for the batch and adds it to the total loss\n",
    "            testingLoss += F.cross_entropy(predictions, target).item()\n",
    "            \n",
    "            #gets the most likely classification for each image\n",
    "            _, predictedValues = torch.max(predictions, 1)\n",
    "\n",
    "            #adds the number of images in the current batch to the total\n",
    "            total += target.size(0)\n",
    "            \n",
    "            #adds the number of correctly classified images to the total correct\n",
    "            correct += (predictedValues == target).sum().item()\n",
    "            \n",
    "    print(f\"Test set loss: {testingLoss/len(testData)}\")\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs the model for 2 epochs\n",
    "for epoch in range(1, 3):\n",
    "    print(f\"Epoch Number {epoch}\")\n",
    "    train()\n",
    "    test()\n",
    "    #saves the weights for later use\n",
    "    torch.save({'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict()\n",
    "               }, 'weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a new ClassifierNet\n",
    "model = ClassifierNet()\n",
    "\n",
    "#loads the saved weights into the new ClassifierNet\n",
    "checkpoint = torch.load('weights.h5')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#tests the saved weights\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
